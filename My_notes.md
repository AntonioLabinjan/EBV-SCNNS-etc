#### Ovaj file Ä‡u koristit za pisanje biljeÅ¡ki i zapaÅ¾anja kad krenen detaljnije istraÅ¾ivat materiju i Äitat resources koje iman

Neuromorphic Electronic Systems, Carver Mead, 1990
- https://hasler.ece.gatech.edu/Published_papers/Technology_overview/MeadNeuro1990.pdf

- PotroÅ¡nja energije je velik problem kod raÄunala
- ObiÄna raÄunala ne mogu imitirati mozak, pa ni njegove najjednostavnije procese
- Mozak je i 10 milijuna puta efikasniji nego raÄunala iz 90-ih
- Funkcije u mozgu se mogu djeliti na: jednostavne elementarne funkcije, reprezentacije informacija i najkompleksnije organizacijske principe 
- Ideja je pokuÅ¡ati reverse-engineerati mozak
- Kompleksno je rastavljati fizikalne principe u bitove i onda ih rekreirati s AND i OR gatesima
- Mozak se moÅ¾e gledati kao distribuirani sustav s viÅ¡e nodesa koji primaju elektriÄne impulse
- Sigurno se radi o nelinearnim funkcijama
- Napon u sinapsama neurona za osnovne funkcije raste eskponencijalno s naponom
- Mozak ima i sustave za dugotrajno uÄenje i memoriju
- Dobro izgraÄ‘en sustav moÅ¾e simulirati uÄenje i pamÄ‡enje (silicon chips)
- Neuromorphic sustavi su sustavi koji implementiraju iste operacija poput Å¾ivÄanog sustava
- Zero-cost addition using Kirchhoff's Law

Kirchhoffov zakon (struja): u jednom Ävoru zbroj svih struja koje ulaze = zbroj svih struja koje izlaze.

Ako viÅ¡e izvora struje spojiÅ¡ na isti Ävor â†’ automatski se saberu (nema dodatnih komponenti).

Zero-cost addition = zbrajanje bez dodatnog sklopa â†’ priroda elektronike sama radi raÄunanje.

Kapacitet Ävora (parazitski ili namjerni kondenzator) integrira ukupnu struju tijekom vremena â†’ dobijeÅ¡ napon koji predstavlja zbroj.

U digitalnim sklopovima bi ti za zbrajanje trebao adder (logika, tranzistori, energija).

U neuromorfnim sklopovima: samo spojiÅ¡ sinaptiÄke struje na Ävor i dobijeÅ¡ sabiranje â€œbesplatnoâ€.

To je ultra-bitno jer neuron = integrator sinaptiÄkih ulaza â†’ Kirchhoff + kapacitet daju ti toÄno tu funkcionalnost prirodno.

Mead je rekao: iskoristi fiziku (Kirchhoffov zakon) umjesto da je â€œnadograÄ‘ujeÅ¡â€ kompleksnim digitalnim adderima.

ğŸ‘‰ Ukratko: viÅ¡e sinapsi Å¡alje spikeove kao struje â†’ struje se automatski zbroje na membrani neurona â†’ kondenzator integrira â†’ izlaz = ponaÅ¡anje bioloÅ¡kog neurona, i to praktiÄki bez troÅ¡ka.

Evo kako ide â€œstoryâ€ u neuromorfnom hardveru i u mozgu:

ViÅ¡e sinapsi Å¡alje svoje â€œsignaleâ€ (u elektronici kao struje, u mozgu kao ionski tokovi).

Te sve struje ulaze u jedan Ävor (membrana neurona).

Po Kirchhoffovom zakonu â†’ struje se automatski zbroje (niÅ¡ta ne moraÅ¡ posebno raÄunati).

Kondenzator membrane integrira taj ukupni signal kroz vrijeme â†’ to daje membranski potencijal.

Ako potencijal preÄ‘e odreÄ‘eni threshold â†’ neuron ispali spike (akcijski potencijal).

ğŸ‘‰ Ukratko: Neuron radi kao prirodni zbrajaÄ + integrator.

Sabira ulaze (Kirchhoff).

Pamti ih kroz vrijeme (kapacitor = integracija).

Triggera izlaz kad sve skupa dosegne kritiÄnu vrijednost (threshold).

- Zjenica oka se, pri direktom osvjetljenju Äini svjetlijom/tamnijom u odnosu na okolinu, ovisno o okolini
- Zjenica stvara napon po primitku inputa (svjetlosti)
- RaÄuna se prosjek prostorno weighted inputa (ne vrijedi svaki input jednako, ovisno o poziciji)
- Cilj je napraviti senzor sliÄan oku koji moÅ¾e reagirati na sve inpute, neovisno od kud dolaze i koji im je intenzitet (zato se koriste weightovi)
- Koriste se amplifieri kako bi se bolje simulirao pravi potencijal Å¾ivÄanog sustava
- Laplacian filter => edge detction filter

Ajmeee, top Å¡to pitaÅ¡ za Laplacian filter! ğŸ”¥ To ti je jedan od najpoznatijih **edge detection filtera** u raÄunalnom vidu i obradi slika.

Evo ti jasno u natuknicama:

---

### ğŸ”¹ Å to je Laplacian filter?

* **MatematiÄki temelj:** koristi **Laplacian operator** (druga derivacija funkcije).
* **Poanta:** pronalazi mjesta gdje se intenzitet slike brzo mijenja â†’ tj. rubove (*edges*).
* Za razliku od Sobel/Prewitt (koji koriste prvu derivaciju i detektiraju smjer ruba), Laplacian gleda **drugu derivaciju** â†’ pa je osjetljiv na *naglu promjenu nagiba*.

---

### ğŸ”¹ Kako radi na slici?

* Sliku gleda kao **matricu piksela**.
* Na svaki piksel primjenjuje kernel (konvolucijsku masku) koji aproksimira Laplacian operator.
* TipiÄni kernel izgleda npr. ovako:
Laplacian filter kerneli:
```
0  -1   0
-1  4  -1
0  -1   0
```

i "jaÄa" verzija:
```
-1  -1  -1
-1   8  -1
-1  -1  -1
```

* Kad se primijeni, filter daje visoke vrijednosti na mjestima gdje postoji jaka promjena intenziteta â†’ tj. rub.

---

### ğŸ”¹ ZaÅ¡to je koristan?

* Detektira rubove **neovisno o smjeru** (Sobel detektira horizontalne/vertikalne, Laplacian je â€œisotropicâ€).
* Jako dobar za **naglaÅ¡avanje struktura** i **detekciju kontura**.
* ÄŒesto se koristi nakon blura (npr. Gaussian blur + Laplacian = *Laplacian of Gaussian*, LoG) â†’ to pomaÅ¾e da se smanji Å¡um.

---

### ğŸ”¹ Intuicija

* Ako piksel izgleda â€œsliÄanâ€ susjedima â†’ rezultat â‰ˆ 0.
* Ako je piksel jako razliÄit od susjeda â†’ rezultat velik (pozitivan ili negativan) â†’ to je rub.

---

ğŸ‘‰ Ukratko: **Laplacian filter je edge detector baziran na drugoj derivaciji, hvata rubove u svim smjerovima i jako se koristi u raÄunalnom vidu.**


PRIMJER PRIMJENE by ChatGPT:

Ajmo brutalno jednostavno da odmah sjedne ğŸ”¥

---

### ğŸ“Œ Primjer matrice (mala â€œslikaâ€ 5Ã—5 piksela)

Zamisli da imaÅ¡ grayscale sliku, gdje su vrijednosti piksela:

```text
10  10  10  10  10
10  10 100  10  10
10  10 100  10  10
10  10 100  10  10
10  10  10  10  10
```

ğŸ‘‰ To ti je slika gdje imaÅ¡ **vertikalnu bijelu crtu** (100) na sredini crne pozadine (10).

---

### ğŸ“Œ Laplacian kernel (klasiÄni)

```text
0  -1   0
-1  4  -1
0  -1   0
```

---

### ğŸ“Œ Primjena filtera (konvolucija)

1. Uzimamo kernel 3Ã—3 i stavljamo ga iznad piksela u centru (recimo sredina je `100`).
2. PomnoÅ¾imo element po element i zbrojimo:

```
Kernel:          Dio slike:       Rezultat = sum(kernel * dio_slike)

 0  -1   0       10  100  10
-1   4  -1   *   10  100  10   =   (0*10) + (-1*100) + (0*10) +
 0  -1   0       10  100  10       (-1*10) + (4*100) + (-1*10) +
                                    (0*10) + (-1*100) + (0*10)

= -100 -10 +400 -10 -100
= 180
```

ğŸ‘‰ Dakle u centru bijele crte izlazni piksel je **jako velik (180)** â†’ filter ga oznaÄi kao rub.

3. Ako isti kernel stavimo na dio pozadine (gdje su sve vrijednosti 10):

```
 0  -1   0       10  10  10
-1   4  -1   *   10  10  10   = (0*10) + (-1*10) + (0*10) +
 0  -1   0       10  10  10      (-1*10) + (4*10) + (-1*10) +
                                  (0*10) + (-1*10) + (0*10)

= -10 -10 +40 -10 -10
= 0
```

ğŸ‘‰ Rezultat = **0** â†’ pozadina nema rub, pa filter niÅ¡ta ne detektira.

---

### ğŸ“Œ ZakljuÄak

* Na rubu (gdje se vrijednost naglo mijenja) dobijeÅ¡ **visok rezultat**.
* Na ravnom dijelu slike â†’ rezultat â‰ˆ **0**.

---

Primjenjuje se na cijelu sliku, dio po dio
Evo kako to ide u praksi:

ImaÅ¡ cijelu sliku (npr. 512Ã—512 piksela).

Uzmemo kernel (mala matrica, npr. 3Ã—3 ili 5Ã—5).

Kernel â€œkliziâ€ po slici â€“ piksel po piksel, red po red (to se zove konvolucija).

Za svaki poloÅ¾aj:

UzmeÅ¡ â€œkomadiÄ‡â€ slike iste veliÄine kao kernel.

PomnoÅ¾iÅ¡ element po element (kernel Ã— slika).

ZbrojiÅ¡ sve rezultate â†’ to je novi izlazni piksel u filtriranoj slici.

Rezultat cijelog procesa = nova slika gdje su naglaÅ¡eni rubovi (ili Å¡to god kernel radi).

ğŸ‘‰ Dakle, da, filter se primjenjuje na cijelu sliku, dio po dio, i rezultat je feature map (slika s naglaÅ¡enim znaÄajkama).

- PomaÅ¾e u lokalizaciji objekata (odredimo di su u prostoru pomoÄ‡u edge detectiona)
- Bitno je napravit da se broj kalkulacija u oku/mozgu prilagodi broju vanjskih evenata (MOZAK NE PROCESIRA ISTU KOLIÄŒINU PODATAKA AKO GLEDA U BIJELI ZID I AKO -GLEDA U NEÅ TO POKRETNO I DINAMIÄŒNO) => dali je to maybe event-based princip??
- Cilj: viÅ¡e kalkulacija za viÅ¡e evenata; manje kalkulacija za manje evenata
- VaÅ¾nost napona pojedinog eventa slabi s protokom vremena i dolaskom novih evenata
- Ni jedan senzor u oku i mozgu nije 100% isti kao neki drugi i zato koriste adaptivne mehanizme da kompenziraju manjak preciznosti
- U mraku zjenica ne mora niÅ¡ procesirat jer ne vidi niÅ¡
- Uzima se input. Radi se predikcija inputa (Äa mozak misli da vidi). Ako je ok, ne dela nikakav dodatan tuning. Ako pogrijeÅ¡i, mora opet kalkulirat i malo prilagodit parametre (primjer: zjenice se Å¡ire u tami da bolje vidimo)
- Isti princip je i za situaciju: neÅ¡to gledamo i delamo predikciju Äa Ä‡e se desit. Ako pogrijeÅ¡imo, radi se tuning, ako pogodimo, sve je ok
- Adaptation feedback
- Mozak raÄuna average od svih viÄ‘enih piksela da se bolje adaptira i da vidi bitno, a izbacuje nebitno
- Problem simuliranog mozga pomoÄ‡u Äipova: ako 1 Äip crkne, sve zgori. Stvara se jako puno topline i troÅ¡i se puno energije
- NaÅ¡ mozak nije u potpunosti spojen (x nodeova spojeno sa x nodeova, nego su spojeni lokalno i onda se to nadoveÅ¾e, ali ne svaki sa svakin, jer bi to zauzimalo previÅ¡e mista)
- Lokalno spajanje je kljuÄno za efikasnost
- Ako imamo nepravilne inpute, treba nam neuromorphic computing 
- I do 10x efikasnije u performansama + 10000 x manje power consumptiona


 A 128x128 120 db 15 Âµs Latency Asynchronous Temporal Contrast Vision Sensor, Patrick Lichsteiner, Cristopher Posch, Tobi Debruck
- https://www.ifi.uzh.ch/dam/jcr:18928364-d16b-4403-9217-2098aaad72bd/lichtsteiner_dvs_jssc08.pdf
- 128x128 pikselni CMOS vision senzor
- Svaki piksel zasebno i kontinuirano generira spike evente na temelju lokalnih promjena intenziteta
- brÅ¾i od milisekunde
- "mreÅ¾nica od silcija"

- PROS and CONS FRAME BASED VISIONA
- PROS: mali i jednostavni pikseli -> visoka rezolucija
                                   -> viskok fill factor i nizak imager cost (istraÅ¾it dodatno)
- CONS: - baziran na serijama snapshota -> pikseli se ponovno obraÄ‘uju Äak i ako su nepromijenjeni iz framea u frame -> JAKO BITNO
        - limitirani bandwith (FRAME RATE/2)
        - limitirani dynamic range (ograniÄen broj piksela po frameu)

IDEJA: senzor koji prati promjene inteziteta u okolini i pretvara ih u asinkrone promjene piksela
output: asinkroni stream pixel address evenata (AEs) => smanjuje se redundantnost (ignoriramo nepromijenjene piksele)
imitiranje bioloÅ¡kog vida i odbacivanje frame-based principa

KONCEPT ADDRESS-EVENT: svaki piksel dobije svoju koordinatnu adresu (lokalni array)
                       kontinuirano; sliÄno kao moÅ¾dani impulsi
                       jako high speed

PROBLEM S PROTOTIPOVIMA: uopÄ‡e ne smanjuju redundantnost podataka, nego samo prostornu redundantnost
- puno noisea
- spori response
- limitirani dynamic range
- mala osjetljivost na kontrast

POLJE JE BILO NERAZVIJENO ZBOG NEZNANJA O ASINKRONOSTI I ZBOG BEZNAÄŒAJNE UNIFORMNOSTI RESPONZIVNIH KARAKTERISTIKA PIKSELA

1) AER senzor -> MAHOWALD I MEAD
   - silikonska mreÅ¾nica s adaptivnim fotoreceptorima, spatial smoothing mreÅ¾om i self-timed komunikacijom
   DEMO UREÄAJ BEZ IKAKVE REAL-WORLD PRIMJENE

2) Spatial+temporal filtering -> ZAGHOUL I BOAHEN
   - puno malih tranzistora i el. krugova usko spojenih diffusor networksima
   - veliki mismatch -> GOTOVO 50% PIKSELA NE OSTVARUJE SPIKE

3) CSEM NEUCHATEL -> ostvaren spatial kontekst -> transisija evenata high-low
  - moguÄ‡e zaustaviti ranije
  - ne smanjuje redundantnost, ograniÄena rezolucija zbog frame ratea

4) CULURCIELLO-ANDREON -> inter-event interval / mean frequency
   (+) - mala koliÄina piksela
   (-) - bandwith ovisi o osvjetljenju
   nema reset mehanizma (puno tamnih piskela)

5) ETIENNE-CUMMING
   - temporal change detection imager
   - moÅ¾e detektirati apsolutnu promjenu osvjetljenja
   - sinkrono -> FIFO pristup
  
VISION SENSOR DESIGN
a) PIXEL DESIGN
 - cilj je: minimizacija mismatcheva
 - Å¡iroki dynamic range
 - low latency
BRZI LOGARITAMSKI PHOTORECEPTOR CIRCUIT -> pojaÄava promjene
- kontrolira pojedine piksele + brzo odgovara/reagira na promjene osvjetljenja
- mana: tranzistori uzrokuju mismatch meÄ‘u pikselima zbog varijacija u thresholdu
- potrebna je dodatna kalibracija
KALIBRACIJA : balansiranje outputa na reset level nakon generiranja eventa (nekon Äa se zabiljeÅ¾i event, senzor se resetira => fadeout u onima mojima testnima kodovima...)
!differencing circuit! -> pikseli su osjetljivi na temporal contrast -> ima formula u paperu; bitno

RECEPTOR CIRCUIT SADRÅ½I: fotodiodu, tranzistor, amplifier -> "TRANSIMPEDANCE CONFIGURATION" -> logaritamska pretvorba photocurrenta u NAPON

PoveÄ‡an bandwith -> veÄ‡a brzina
adaptive biasing -> znatno manja potroÅ¡nja energije
reset switch -> OKIDA input i output zajedno i tako resetira napon

OVO OPISUJE ONAJ PROCES KADA SE EVENT POJAVI NA KAMERI I ONDA SAMO IZBLJEDI I RESETIRA SE

ON i OFF eventi -> circuit ih emitira u periferiju
Pixel more emitat ili ON ili OFF event (NIKAD OBA ISTOVREMENO) prema periferiji
Proces "komunikacije" kreÄ‡e kad event trigerira piksel, a zavrÅ¡ava kada se piksel resetira na 0 (vrati u poÄetno stanje)
Prijevod na glupo: ON event -> neÅ¡to se miÄe i pikseli reagiraju; OFF event -> niÅ¡ se viÅ¡e ne miÄe, pikseli miruju

ograniÄava se "fire-rate" za piksele kako pojedini pikseli ne bi zauzeli sve resurse za obradu
omoguÄ‡ava da se viÅ¡e resursa posveti aktivnijim podruÄjima (procesiraÅ¡ ono di se neÅ¡to deÅ¡ava; di ima evenata)
Cilj je da se circuit ne pregrijava pretjerano, da ne troÅ¡i resurse nepotrebno, da radi samo kad se neÅ¡to dogaÄ‘a

KARAKTERISTIKE NAJVAÅ½NIJIH ASPEKATA:

Uniformity of response - vraÄ‡a razliku u osvjetljenju piksela -> bitno je uniformno i standardizirano evaluirati promjene
Dynamic range - razlika imzeÄ‘u maksimalnog i minimalnog osvjetljenja u "sceni" kroz piksele; reliable and reproducible events
Pixel bandwith - iznos raspona jaÄine najjaÄeg i najslabijeg intenziteta koji 1 piksel "prepozna"/"odradi". Stariji i noviji eventi se razlikuju u intenzitetu; intenzitet opada protokom vremena (fade-out)
Latency&Latency jitter - ideja: Äa je osvjetljenje manje, latencija je veÄ‡a => proporcionalna reciproÄnom osvjetljenju (Äa je Å¡kurije, teÅ¾e se skuÅ¾e eventi i promjene u svjetlosti kod piksela)
UtvrÄ‘eno je kako je latencija ipak jako mala u svim uvjetima (SUPER ZA REAL-TIME PROCESSING)


### Synaptic modifications in cultured hippocampal neurons:
### Dependence on spike timing, synaptic strength and postsynaptic cell type
### Guo-qiang Bi&Mu-ming Poo

Eksperimenti raÄ‘eni na hipokampusnim neuronima Å¡takora
PostsinaptiÄki i presinaptiÄki neuroni

PresinaptiÄki neuron = onaj koji Å¡alje signal.

Njegov akson zavrÅ¡ava na sinapsi i otpuÅ¡ta neurotransmitere (npr. glutamat).

To je â€œpoÅ¡iljatelj porukeâ€.

PostsinaptiÄki neuron = onaj koji prima signal.

Na svojim dendritima ima receptore koji hvataju neurotransmitere i reagira (moÅ¾e spajkati ako se dovoljno pobudi). => procesira neÅ¡to samo ako dobije dovoljno inputa i skuÅ¾i da se neÅ¡to desilo

To je â€œprimatelj porukeâ€.

Spike neurona => registrira dovoljno ulaznih signala i preÄ‘e threshold aktivacije => ako je input dovoljno "jak", aktivira se spike
Okine strujni elektriÄni impuls (par milisekundi) i otpuÅ¡ta neurotransmitere

U kontekstu event based visiona => spike je event
Nema stalnih signala kao kod obiÄnih CNN-ova nego samo triger kad se dogodi dovoljno jaka promjena
TroÅ¡i se energija samo kada se neÅ¡to stvarno dogaÄ‘a
Spike => kratki elektriÄni event kojim neuron javlja da je aktivan
Npr. Ako se detektira dovoljno pokreta, onda se input procesira, a ako ima malo/niÅ¡ pokreta, ne procesiramo niÅ¡

LTP i LTD = dugotrajno jaÄanje ili slabljenje sinapsi, inducirano ponavljanom elektriÄnom aktivnoÅ¡Ä‡u.

KlasiÄni protokoli: obiÄno ukljuÄuju repetitivnu presinaptiÄku stimulaciju (razne frekvencije) + Äesto dodatnu depolarizaciju postsinaptiÄkog neurona.

U nekim eksperimentima trebalo je blokirati spontane spikeove (npr. tetrodotoksin) ili smanjiti MgÂ²âº da bi LTP uspio.

Novija istraÅ¾ivanja (in vitro slices) pokazala su da precizno tempirani back-propagating action potentials (postsynaptic spike koji se vraÄ‡a kroz dendrite) mogu inducirati LTP ili LTD.

KljuÄna nova ideja: nije samo vaÅ¾na frekvencija stimulacije â†’ relativno vrijeme izmeÄ‘u presinaptiÄkog i postsinaptiÄkog spikea odluÄuje smjer plastiÄnosti.
Ni bitno koliko je 1 event jak, nego je za spike bitno da se deÅ¡ava viÅ¡e evenata u kratkom vremenu

Rezultati ove studije:

PostsinaptiÄki spike <20 ms nakon presinaptiÄkog â†’ LTP.

PostsinaptiÄki spike <20 ms prije presinaptiÄkog â†’ LTD.

Postoji uska prijelazna zona od samo ~5 ms izmeÄ‘u LTP i LTD.

Ovisnost o poÄetnoj snazi sinapse:

Slabe sinapse = visoka Å¡ansa za LTP.

Jake sinapse = manje podloÅ¾ne daljnjem jaÄanju.

SpecifiÄnost ciljne stanice:

GlutamatergiÄke sinapse na GABA neurone ne pokazuju ovu vrstu plastiÄnosti.

Implicira target cellâ€“specific mehanizme.


---

* **LTP i LTD** = dugotrajno jaÄanje ili slabljenje sinapsi, inducirano ponavljanom elektriÄnom aktivnoÅ¡Ä‡u.
* **KlasiÄni protokoli**: obiÄno ukljuÄuju **repetitivnu presinaptiÄku stimulaciju** (razne frekvencije) + Äesto dodatnu depolarizaciju postsinaptiÄkog neurona.

  * U nekim eksperimentima trebalo je blokirati spontane spikeove (npr. tetrodotoksin) ili smanjiti MgÂ²âº da bi LTP uspio.
* **Novija istraÅ¾ivanja (in vitro slices)** pokazala su da **precizno tempirani back-propagating action potentials** (postsynaptic spike koji se vraÄ‡a kroz dendrite) mogu inducirati LTP ili LTD.
* **KljuÄna nova ideja**: nije samo vaÅ¾na frekvencija stimulacije â†’ **relativno vrijeme izmeÄ‘u presinaptiÄkog i postsinaptiÄkog spikea odluÄuje smjer plastiÄnosti**.
* **Rezultati ove studije**:

  * PostsinaptiÄki spike **<20 ms nakon** presinaptiÄkog â†’ **LTP**.
  * PostsinaptiÄki spike **<20 ms prije** presinaptiÄkog â†’ **LTD**.
  * Postoji uska prijelazna zona od samo **~5 ms** izmeÄ‘u LTP i LTD.
* **Ovisnost o poÄetnoj snazi sinapse**:

  * **Slabe sinapse** = visoka Å¡ansa za LTP.
  * Jake sinapse = manje podloÅ¾ne daljnjem jaÄanju.
* **SpecifiÄnost ciljne stanice**:

  * GlutamatergiÄke sinapse na **GABA neurone** ne pokazuju ovu vrstu plastiÄnosti.
  * Implicira **target cellâ€“specific mehanizme**.

STDP je bioloÅ¡ki mehanizam uÄenja kojim se jaÄina veze (sinaptiÄka teÅ¾ina) izmeÄ‘u dva neurona mijenja ovisno o vremenskom razmaku izmeÄ‘u njihovih spikeova (akcijskih potencijala).

Drugim rijeÄima:
Nije samo vaÅ¾no da neuroni spajkaju zajedno, nego toÄno kada to rade.

Pravilo (u osnovnoj formi):

PresinaptiÄki neuron spajka PRVI â†’ PostsinaptiÄki ubrzo nakon (do ~20 ms)

LTP (Long-Term Potentiation): sinapsa se ojaÄa.

ZnaÄenje: presinaptiÄki neuron je vjerojatno pridonio aktivaciji postsinaptiÄkog â€“ mozak to â€œnagradiâ€ jaÄanjem veze.

PostsinaptiÄki neuron spajka PRVI â†’ PresinaptiÄki tek kasnije (do ~20 ms)

LTD (Long-Term Depression): sinapsa se oslabi.

ZnaÄenje: presinaptiÄki neuron nije uzrokovao aktivaciju postsinaptiÄkog, pa mozak smatra da je ta veza nebitna.

Ako su spikeovi previÅ¡e udaljeni u vremenu (> ~20 ms)

 Nema promjene â€“ mozak zakljuÄuje da nema uzroÄno-posljediÄne veze.
 
 U mozgu (STDP):

Ako presinaptiÄki spike doÄ‘e, i postsinaptiÄki spike se dogodi ubrzo nakon â†’ mozak zakljuÄi da ovaj input uzrokuje ovaj output.

Rezultat = ojaÄaj tu sinapsu (nauÄi asocijaciju).

Ako je obrnuto (postsinaptiÄki prije presinaptiÄkog) â†’ veza se smatra nebitnom â†’ oslabi sinapsu.

Analogija s event-based kamerom:

Ako se u kratkom vremenu i prostoru javi viÅ¡e evenata â†’ sustav zakljuÄuje da pripadaju istoj stvari (npr. kretanje osobe).

Ako eventi dolaze previÅ¡e razdvojeni u vremenu/prostoru â†’ nisu povezani, ignoriraÅ¡ ih.

1. â€œPostsinaptiÄki spike slijedi EPSP unutar ~15 ms â†’ LTPâ€

EPSP (Excitatory Post-Synaptic Potential) = mali elektriÄni signal u postsinaptiÄkom neuronu, nastao kad presinaptiÄki neuron otpusti glutamat.

Ako taj EPSP uspije â€œpoguratiâ€ postsinaptiÄki neuron do praga i on okine spike vrlo brzo nakon toga (~15 ms) â†’ mozak zakljuÄi:

â€œOvaj presinaptiÄki input je doprinos postsinaptiÄkoj aktivaciji.â€

Rezultat: ojaÄaj tu sinapsu (LTP).

To je zapravo bioloÅ¡ki dokaz Hebbove ideje: neuroni koji spajkaju zajedno, povezuju se zajedno, ali uz precizan timing.

2. â€œI slabe i jake sinapse mogu se ojaÄati ako je spike timing pozitivno koreliranâ€

â€œSlaba sinapsaâ€ = EPSP sam po sebi nije dovoljan da izazove spike u postsinaptiÄkom neuronu.

â€œJaka sinapsaâ€ = EPSP dovoljno velik da sam digne neuron do spikea.

Pokus pokazao: i slabe i jake veze se mogu ojaÄati, ako se pobrineÅ¡ da postsinaptiÄki spike padne unutar tog prozora (npr. kod slabih sinapsi su ruÄno injektirali malu struju da potaknu spike nakon EPSP).

ZakljuÄak: bitan je timing, a ne samo jaÄina veze.

â€œNMDA receptori su kljuÄniâ€

NMDA receptor = poseban tip glutamatnog receptora koji je osjetljiv na napon i omoguÄ‡uje ulazak CaÂ²âº iona u neuron.

Taj CaÂ²âº signal je â€œmolekularni prekidaÄâ€ koji pokreÄ‡e plastiÄnost (LTP/LTD).

Kad su ih blokirali s lijekom D-AP5 â†’ nije bilo LTP-a â†’ dakle, bez NMDA nema uÄenja.

LTP (long-term potentiation) = trajno jaÄanje sinapse (poveÄ‡ava se vjerojatnost da Ä‡e signal proÄ‡i).

EPSP (excitatory postsynaptic potential) = mali "plusiÄ‡" napona na postsinaptiÄkom neuronu, kao mini podraÅ¾aj.

Evo paralela s event-based kamerom 

---

### U mozgu (neuroni):

* **PresinaptiÄki spike** = dolazni signal â†’ EPSP.
* **PostsinaptiÄki spike** = izlaz neurona.
* **STDP / LTP** = ako ulazni event stigne malo prije nego neuron ispali, veza se jaÄa.
* **NMDA receptor** = â€œgatekeeperâ€ koji kaÅ¾e: *event se raÄuna samo ako su se ulaz i izlaz poklopili u vremenu*.

---

### U event-based kameri:

* **Pixel event** = presinaptiÄki spike (svaki event je mala promjena â†’ mini EPSP).
* **Kamera/algoritam detektira pokret** = postsinaptiÄki spike (output).
* **STDP logika**:

  * Ako viÅ¡e pixela (ulaznih evenata) **u kratkom vremenu i prostoru** â€œokineâ€ i to dovede do registriranog pokreta (output), sustav zakljuÄi: *ti eventi su povezani, pojaÄaj im znaÄaj*.
* **NMDA paralela** = kao filter koji ne puÅ¡ta â€œrandom Å¡umâ€, nego traÅ¾i da se viÅ¡e evenata poklopi u pravom trenutku â†’ tek onda priznaje da je stvarno detekcija.

---

 Ukratko:
U mozgu â€“ sinapsa se jaÄa ako ulaz brzo dovede do izlaza.
U kameri â€“ grupa evenata se tretira kao znaÄajan pokret samo ako se prostorno-vremenski poklope i generiraju detekciju.

---

Pozitivno korelirani spikeovi (pre â†’ post, unutar ~20 ms)
â†’ rezultira LTP (ojaÄavanje sinapse).

Negativno korelirani spikeovi (post â†’ pre, unutar ~20 ms)
â†’ rezultira LTD (slabljenje sinapse).

Prozor djelovanja (temporal window):

Potencijacija = post-sinaptiÄki spike unutar +20 ms nakon EPSP.

Depresija = post-sinaptiÄki spike unutar âˆ’20 ms prije EPSP.

Izvan Â±40 ms = nema znaÄajne promjene.

NMDA receptori su obavezni i za LTP i za LTD (bez njih nema sinaptiÄke plastiÄnosti).

JaÄina poÄetne sinapse utjeÄe na plastiÄnost:

Slabe sinapse = lakÅ¡e jaÄaju (LTP).

Jako jake sinapse = manje podloÅ¾ne daljnjem jaÄanju.

Tip postsinaptiÄkog neurona:

GlutamatergiÄki = pokazuju STDP (LTP/LTD).

GABA neuroni = nema STDP efekta.

Uloga CaÂ²âº kanala:

L-type CaÂ²âº kanali pomaÅ¾u kod LTP, ali nisu nuÅ¾ni.

Za LTD (depresiju) jesu nuÅ¾ni.

STDP je asimetriÄan:

Ako ulaz doÄ‘e prije izlaza â†’ uÄenje = â€œovo je korisnoâ€ (LTP).

Ako izlaz doÄ‘e prije ulaza â†’ uÄenje = â€œovo nije korisnoâ€ (LTD).
Sve to ovisi o NMDA receptorima, vremenskom prozoru (~Â±20 ms) i jaÄini same sinapse.

Ako se viÅ¡e evenata dogodi vrlo brzo jedan nakon drugog (unutar â€œSTDP prozoraâ€ od ~20 ms), sustav ih tretira kao povezane.

Kao posljedica, postsynaptiÄki spike je jaÄi nego kada bi ti eventi doÅ¡li pojedinaÄno i razdvojeni u vremenu.

BioloÅ¡ki mozak â†’ pojaÄava sinapsu (LTP).

Event-based sustav â†’ grupira dogaÄ‘aje i â€œpojaÄava signalâ€, Å¡to olakÅ¡ava detekciju stvarnih objekata/pokreta.

Ok, ovo je sad super saÅ¾eto i profesionalno â€“ ovo su **kljuÄni zakljuÄci cijelog rada**, izvuÄeni za tvoj event-based vision kontekst:

---

### KljuÄne toÄke / natuknice:

1. **STDP vremensko pravilo je vrlo precizno i usko**:

   * Potencijacija (LTP) se dogaÄ‘a kada **postsynaptiÄki spike slijedi EPSP unutar +20 ms**.
   * Depresija (LTD) se dogaÄ‘a kada **postsynaptiÄki spike prethodi EPSP unutar âˆ’20 ms**.
   * Izvan Â±40 ms â†’ nema znaÄajne sinaptiÄke promjene.
   * Prelaz izmeÄ‘u LTP i LTD je **oÅ¡tra, samo ~5 ms**.

2. **JaÄina sinapse utjeÄe na plastiÄnost**:

   * Slabe sinapse = lako potenciraju (LTP).
   * Jako jake sinapse = mogu biti â€œsaturiraneâ€ i ne reagiraju viÅ¡e na pozitivno korelirane spikeove.

3. **Tip postsinaptiÄkog neurona je kljuÄan**:

   * GlutamatergiÄki neuroni â†’ pokazuju LTP i LTD.
   * GABAergiÄki neuroni â†’ nema sinaptiÄke modifikacije (nije osjetljivo).

4. **NMDA receptori i CaÂ²âº su kritiÄni**:

   * NMDA receptori â†’ obavezni i za LTP i za LTD.
   * L-type CaÂ²âº kanali â†’ potrebni za LTD, pomaÅ¾u za LTP.
   * Lokalni CaÂ²âº spikeovi unutar dendrita omoguÄ‡uju preciznu temporalnu detekciju.

5. **Temporalni slijed spikeova implementira Hebbovu logiku**:

   * Ako **ulaz (presynaptiÄki spike) uzrokuje izlaz (postsynaptiÄki spike)** â†’ veza se jaÄa (LTP).
   * Ako je redoslijed obrnut â†’ veza se slabije aktivira (LTD).
   * Ovo je prirodni â€œcausality detectorâ€ â†’ neuronska mreÅ¾a moÅ¾e uÄiti sekvence i predviÄ‘ati dogaÄ‘aje.

6. **Spontana aktivnost moÅ¾e â€œsaturiratiâ€ sinapse** â†’ starije sinapse u kulturi moÅ¾da ne mogu dodatno potencirati.

7. **Implications for event-based systems:**

   * **Brzo slijedeÄ‡i eventi â†’ tretiraju se kao povezani â†’ jaÄi output**.
   * Sustavi poput spiking CNN-a mogu koristiti STDP logiku da detektiraju obrasce u vremenskim eventima i odluÄe Å¡to je relevantno.
   * Temporalna asimetrija omoguÄ‡uje â€œprediktivno kodiranjeâ€ â†’ sliÄno onome Å¡to mozak radi kod navigacije i sekvencijalnog uÄenja.



A million spiking-neuron integrated circuit with a scalable communication network and interface
Paul A.Merolla&co.

- inspirirano strukturom mozga
- efikasna, skalabilna, fleksibilna, non-von Neumannova arhitektura
- suvremena silikonska tehnologija
- Äip od 5.4 milijardi tranzistora s 4096 neurosinaptiÄkih jezgri meÄ‘usobno povezanih preko intrachip networka
- network se sastoji od milijun programabilnih spiking neurona i 256 milijuna konigurabilnih sinapsi (sinapse su veze meÄ‘u neuronima; zato ima viÅ¡e sinapsi nego neurona jer veze nisu 1:1)
- Äipovi se mogu povezati u 2 dimenzije pomoÄ‡u interchip komunikacijskog interfacea -> skaliranje na razinu cortexlike sheeta proizvoljne veliÄine (cortex => vanjski dio mozga..."kora")
- omoguÄ‡ava koriÅ¡tenje kompleksnih neuronskih mreÅ¾a u realnom vremenu (npr. multiobject detekcija i klasifikacija)
- 400 px x 240px video input brzine 30 fps-a na Äipu troÅ¡i 63 miliwatta
Chatgpt:
Da, ako to stavimo u perspektivu, ovo je **priliÄno efikasno**, pogotovo za embedded ili edge AI Äipove. Hajde da to raÅ¡Älanimo:

* Video ulaz: **400 Ã— 240 px** â†’ 96.000 piksela po okviru.
* Brzina: **30 fps** â†’ 2.880.000 piksela obraÄ‘eno u sekundi.
* PotroÅ¡nja: **63 mW** â†’ 0,063 J po sekundi.

Ako raÄunamo **energiju po pikselu**:

E_per_pixel = 0.063 / (400 * 240 * 30) = 21.875e-9 J = 21.875 nJ/pixel

Za usporedbu, moderni edge AI Äipovi (npr. za male kamere ili IoT senzore) Äesto ciljaju **10â€“100 nJ/piksel** za video obrade pri visokoj efikasnosti.

ZakljuÄak: **63 mW za 400Ã—240 @ 30 fps je vrlo dobar rezultat**. Ovo je tipiÄno u rangu ultra-low-power AI kamera Äipova.

Cilj je stvoriti viÅ¡enamjenski kompjuter efikasan u vidu prostora i potroÅ¡nje resursa, skalabilan za velike neuronske mreÅ¾e i sinapse, dovoljno fleksibilan da odraÄ‘uje bihevioralne modele cortexa mozga

Takva raÄunala 2014. nisu postojalaa

Von Neumannova arhitektura je neefikasna i neskalabilna te ne moÅ¾e kvalitetno reprezentirati masivne, meÄ‘usobno povezane neuronske mreÅ¾e
Koristi se miks analognih i digitalnih neuromorfnih signala => imitacija neurobioloÅ¡kih procesa koriÅ¡tenjem:
- silicijskih neurona
- winner-take-all strujnih krugova (pogledat detaljnije)
- senzorskih strujnih krugova

- Digitalno implementirani spiking neuroni su efikasniji od svih dosadaÅ¡njih dizajna
- omoguÄ‡avaju one-to-one correspondence izmeÄ‘u softvera i hardvera => ÄŒa znaÄi one-to-one?
Svaka funkcija softvera (npr. odreÄ‘eni algoritam, neuronski sloj, procesiranje piksela) ima direktno odgovarajuÄ‡i hardverski resurs koji ju izvrÅ¡ava.
Nema posrednog sloja ili generiÄkog procesora koji simulira funkciju â€” hardware je dizajniran toÄno za tu operaciju.

- stvoreni su event driven komunikacijski frameworkovi => komunikacija se deÅ¡ava samo kad se dogodi event
- na taj naÄin se emulira unutarnja povezanost unutar mozga

Colocated memory and computation + event driven communication nadilazi von Neumann bottleneck

Colocated memory and computation + event driven communication nadilazi von Neumann bottleneck

Von Neumann bottleneck

* KlasiÄna arhitektura raÄunala (CPU + RAM) ima **odvojen procesor i memoriju**.
* Podaci i instrukcije se stalno Å¡alju izmeÄ‘u memorije i CPU-a preko **jedne sabirnice**.
* **Problem:** sabirnica je usko grlo â†’ CPU Äesto Äeka podatke iz memorije â†’ sporije i manje energetski efikasno.
* To je tzv. **von Neumann bottleneck**.

Colocated memory and computation

* â€œColocatedâ€ znaÄi da **memorija i procesiranje stoje zajedno**, na istom Äipu ili blizu.
* Prednost: podatke **ne treba stalno pomjerati preko sabirnice** â†’ manje Äekanja, manja potroÅ¡nja energije.
* Primjer: **neuromorfni Äipovi**, gdje svaki â€œneuronski elementâ€ ima svoje â€œlocal memoryâ€ i logiku za izraÄun.


Event-driven communication

* Sustav **reagira samo kad se dogodi event** (npr. piksel se promijeni).
* Za razliku od stalnog pollanja, **samo se Å¡alju podaci koji su relevantni**.
* Prednost: joÅ¡ manja potroÅ¡nja i brÅ¾i prijenos informacija.


ZakljuÄak

* Kombinacija **lokalne memorije + event-driven komunikacije** znaÄi:

  * Nema stalnog Äekanja na podatke izmeÄ‘u procesora i memorije.
  * Podaci se Å¡alju **samo kad je potrebno**.
* Dakle, sustav **nadilazi von Neumann bottleneck** â†’ brÅ¾e, efikasnije, manje energije troÅ¡i, posebno kod neuromorfnih i edge AI sustava.

KljuÄna arhitektura => mreÅ¾a neurosinaptiÄkih jezgri koja moÅ¾e izgraditi velike spiking neural networke koji su efikasni, skalabilni i fleksibilni

Osnovna graÄ‘evna jedinica je "core" -> samostojeÄ‡a neuronska mreÅ¾a s 256 inputa (aksona) i 256 outputa (neurona) povezanih preko 256x256 usmjerenih, programabilnih sinaptiÄkih veza
Od viÅ¡e coreva mogu se graditi sloÅ¾enije mreÅ¾e pomoÄ‡u globalne, distribuirane on- and off-chip povezivosti
Svaki neuron na svakom coreu moÅ¾e se povezati s bilo koji aksonom na bilo kojem drugom coreu (jako dobra povezanost viÅ¡e neuronskih mreÅ¾a)

Aksionski branching je implementiran hijerarhijski u 2 stadija:

Najprije 1 konekcija putuje od starta (prvog corea) skoro do cilja (drugog corea), ali se po dolasku do aksona razdvaja u puno manjih konekcija koje prolaze joÅ¡ kratku udaljenost unutar samog corea

Neuroni se kreÄ‡u u stepsima od 1ms (orkestrirano pomoÄ‡u globalnog 1-kHz clocka)

Uz opisani globalni sinkronizacijski clock (koji osigurava 1-1 konekciju softvera i hardvera), coresi koriste i paralelni  i event driven naÄin rada
Osnovni napon upravlja potpuno asiknronom intercore komunikacijom i event-driven intracore komunikacijom. Taj napon radi po principu all-or-nothing spike evenata koji predstavljaju aktivaciju pojednih neurona

Efikasno zato jer se neuroni klasteriraju u klastere koji izvlaÄe svoje inpute iz sliÄnih poolova aksona (neuroni se grupiraju prema aksonima)
Samo spike eventi se Å¡alju komunikacijom izmeÄ‘u jezgri . Å to su eventi jaÄi, to se ÄeÅ¡Ä‡e Å¡alju

Ta je arhitektura skalabilna jer jezgre na Äipu, kao i sami Äipovi imaju moguÄ‡nsot slaganja u 2 dimenzije (sliÄno kao cortex sisavaca)
Svaki spike event  cilja toÄno odreÄ‘eni "pool" neuorna na ciljanoj jezgri i tako smanjuje broj long-range spike evenata koji predstavljaju kljuÄni bottleneck (event se ne Å¡alje svuda nego toÄno odreÄ‘enim neuronima koji ga trebaju primiti)
Fault tolerance -> povremene greÅ¡ke u core-u i na Äipovima ne oslabljuju rad cijelog sustava

Arhitektura je fleksibilna jer je svaki neuron individualno konfigurabilan i imitira prave neurone podrÅ¾avajuÄ‡i razne funkcije i bioloÅ¡ki relevantne spiking mehanike
Svaka sinapsa se moÅ¾e individualno "paliti i gasiti" i moÅ¾e joj se dodjeliti relativna snaga (sliÄno kao weighted metrike)
Svaka neuron-axon veza je programabilna s ukljuÄenim axonal delayom
Neuroni i sinapse mogu prikazivati programirano stohastiÄko (nasumiÄno, ali s predvidivim obrascem) ponaÅ¡anje korsteÄ‡i pseudo-random number generator (1 po jezgri)
Arhitektura podrÅ¾ava imitaciju psiholoÅ¡ke dinaimke i anatomske povezanosti.
Ima: feed-forward, reccurent i lateral veze

---

### **1ï¸. Feed-forward veze**

* Informacija ide **jednosmjerno od ulaza prema izlazu**.
* Nema povratnih veza â†’ signal ne vraÄ‡a se unatrag.
* TipiÄno u **klasiÄnim perceptronima i CNN-ovima**.
* Prednost: jednostavno, stabilno, brzo raÄunanje.
* Mana: ne moÅ¾e modelirati vremenske sekvence ili stanje memorije.

---

### **2ï¸. Recurrent veze**

* Informacija se **vraÄ‡a unatrag unutar mreÅ¾e** â†’ stvaraju **petlje**.
* OmoguÄ‡uju mreÅ¾i da **pamti prethodne ulaze** â†’ modeliranje sekvenci, npr. tekst, govor, vremenske serije.
* Primjeri: RNN, LSTM, GRU.
* Mana: teÅ¾e trenirati (problem â€œexploding/vanishing gradientsâ€).

---

### **3ï¸. Lateral veze**

* Veze **izmeÄ‘u neurona unutar istog sloja**.
* OmoguÄ‡uju **modulaciju aktivnosti susjednih neurona**, konkurenciju ili potporu.
* TipiÄno u **bioloÅ¡kim mreÅ¾ama** â†’ lateralna inhibicija pomaÅ¾e kod detekcije rubova, kontrasta, itd.
* U umjetnim mreÅ¾ama rjeÄ‘e, ali se koristi za **attention, normalizaciju ili interneuron modulaciju**.

---

Core ima individualno adressable aksone, konfigurabilni synaptic crossbar array i programabilne neurone

Core (u neuromorfnom Äipu)

Individualno addressable aksone

Svaki aksone (put kojim ide signal od neurona) moÅ¾e se posebno adresirati i kontrolirati.

ZnaÄi moÅ¾eÅ¡ poslati signal toÄno odreÄ‘enom neuronu ili grupi neurona, bez da utjeÄeÅ¡ na ostatak mreÅ¾e.

Konfigurabilni synaptic crossbar array

MreÅ¾a sinapsi (kao matrica) gdje moÅ¾eÅ¡ konfigurirati teÅ¾ine veze izmeÄ‘u neurona.

OmoguÄ‡uje da jedan neuron Å¡alje signal viÅ¡e neurona s razliÄitim jaÄinama â†’ kljuÄno za uÄenje i prilagodbu.

Programabilni neuroni

Neuroni Äipa se mogu programirati: npr. prag aktivacije, dinamiku spike-a, reakciju na inpute.

To omoguÄ‡uje simulaciju razliÄitih tipova bioloÅ¡kih neurona i razliÄitih algoritama unutar istog hardwarea.

Ukratko:

Aksone = kontrola kamo signal ide

Crossbar = kontrola koliko je signal jak izmeÄ‘u neurona

Programabilni neuroni = kontrola kako neuron reagira na signal

Unutar corea informacija teÄe od presinaptiÄkih aksona kroz aktivne sinapse u crossbaru kako bi stvarala inpute za sve povezane postsynaptic neurone

Aksioni se aktiviraju pomoÄ‡u input spike evenata koje generiraju neuroni bilo gdje u sustavu i Å¡alju se dalje nakon definiranog axonal delaya (od 1 do 15 vremenskih jedinica)
Mozak ima dedicated "Å¾icu" za svaku konekciju; Äip ima time-multiplexed Å¾ice koje povezuju 2-d mesh network rutera
Svaki ruter ima 5 portova (sjeverni, juÅ¾ni, istoÄni, zapadni i lokalni)
Ruteri su osnova 2-d mesh networka koji povezuje 64x64 array coresa

Kada neuron u coreu spike-a, u lokalnoj memoriji traÅ¾i axonal delay (4 bita) i adresu destinacije (8 bitova apsolutne adrese za target aksone i dvije 9-bitne relativne adrese koje predstavlaju core "skokove" u svakoj dimenziji ciljanog corea)
Te se informacije enkodiraju u paket koji se Å¡alje pomoÄ‡u mesha od corea do corea (prvo po x-u, pa po y-u) => takva je komunikacije deadlock free
Za implementaciju feedback veza u coreu, gdje se neuron spaja na akson isto corea, paket se dostavlja koristeÄ‡i lokalne kanale rutera (efikasno jer se ne napuÅ¡ta core)
Za skaliranje opisanog sustava (2-d mesh) u kojem je broj interchip konekcija ograniÄen, koristi se merge-split struktura na 4 ruba mesha da bi se postojeÄ‡i spikeovi serijalizirali, a izlazeÄ‡i deserijalizirali. Svakom spikeu se da labela njegovog reda (ako putuje istok-zapad) ili stupca (ako putuje sjever-jug) prije nego se spoje u shared link koji postoji na Äipu
TakoÄ‘er, spikeovi koji ulaze na Äip iz shared linka, razdvajaju se u odgovarajuÄ‡i redak ili stupac koristeÄ‡i info sa tagova

ObjaÅ¡njenje:

---

### 1ï¸.Osnovni tok informacija unutar corea

* **PresinaptiÄki aksone** Å¡alju spike dogaÄ‘aje kroz **aktivne sinapse u crossbar**.
* Crossbar distribuira signal prema **postsynaptiÄkim neuronima** unutar corea.
* **Input spike event** = signal od nekog neurona bilo gdje u sustavu.
* **Axonal delay** = vremensko kaÅ¡njenje prije nego spike stigne do target neurona (1â€“15 jedinica).

**Ukratko:** neuron spike-a â†’ signal ide kroz aksone i sinapse â†’ dolazi do drugih neurona, uz kaÅ¡njenje.

---

### 2ï¸.Razlika izmeÄ‘u mozga i Äipa

* **Mozak:** svaka veza ima svoju â€œÅ¾icuâ€ â†’ fiziÄki dedicated.
* **ÄŒip:** koristi **time-multiplexed Å¾ice** â†’ jedna fiziÄka veza prenosi spikeove od viÅ¡e neurona koristeÄ‡i redoslijed i vrijeme.
* Spikeovi putuju kroz **2-d mesh network**: mreÅ¾a rutera koji povezuju coreove u x i y dimenziji.

---

### 3ï¸.Ruter i 2-d mesh network

* Svaki ruter ima **5 portova**: sjever, jug, istok, zapad, lokalni (core unutar kojeg se nalazi).
* Spike ide kroz mesh tako da prvo ide **po x osi, pa po y osi** â†’ deadlock-free (nema zastoja u mreÅ¾i).
* MreÅ¾a povezuje **64Ã—64 corea**.

---

### 4ï¸.Paket spike-a

* Kada neuron spike-a:

  * TraÅ¾i se **axonal delay** (4 bita).
  * TraÅ¾i se **adresa destinacije** (8-bit apsolutna + 2Ã—9-bit relativne za target core).
  * Ove informacije se enkodiraju u **paket** i Å¡alju kroz mesh.

* **Feedback veze unutar corea** â†’ spike ostaje unutar corea, koristi lokalni kanal rutera â†’ efikasno.

---

### 5ï¸.Skaliranje i interchip komunikacija

* Problem: broj veza izmeÄ‘u Äipova je ograniÄen.

* RjeÅ¡enje: **merge-split struktura**:

  * Spikeovi se **serijaliziraju** prije slanja kroz shared link.
  * Na odrediÅ¡tu se **deserijaliziraju**.
  * Svakom spikeu se dodaje **labela reda (x)** ili **stupca (y)** â†’ omoguÄ‡uje pravilan routing.

* Spikeovi koji dolaze iz shared linka se **razdvajaju u odgovarajuÄ‡i redak/stupac** prema tagu.

---

###  Ukratko, Å¡to se dogaÄ‘a

1. Neuron spike-a â†’ lokalno traÅ¾i delay i adresu.
2. Spike se enkodira u paket â†’ ide kroz 2D mesh network rutera.
3. Ako je intra-core feedback â†’ koristi lokalni kanal, nema routing po meshu.
4. Ako ide na drugi Äip â†’ spikeovi se serijaliziraju, Å¡alju shared linkom, zatim deserializiraju i rasporede po target coreu.

**Cilj:**

* Efikasna, skalabilna, deadlock-free komunikacija izmeÄ‘u 64Ã—64 coreova + interchip povezanost.
* OmoguÄ‡uje **tisuÄ‡e paralelnih spike dogaÄ‘aja** da se prenesu brzo i precizno.

---

Implementacija: milijun spiking neurona i 256 milijuna sinapsi i 5.4 milijarde tranzistora na prostoru od 4.3 cm2
Ima 428 milijuna bitova on-chip memorije
Svaki core ima 104,448 bitova lokalne memorije za pohranu stanja sinapsi (65,536), stanja neurona i parametre (31,232), adrese destinacija (6656) i axonal delays (1024)
Energija: 20 mW po cm2, dok CPU troÅ¡i 50-100 W po cm2
Active power density je nizak zbog arhitekture
Passive power density je nizak zbog tranzistora s malim leakegom

4096 puta viÅ¡e coreva na 15 puta manje povrÅ¡ine, sa 100 puta manje potroÅ¡nje energije u odnosu na prethodna rjeÅ¡enja

OmoguÄ‡ava event driven asinkrono-sinkroni pristup => implementirano koristeÄ‡i: offline learning (CNN, liquid state machines, restricted Boltzman machines, hidden Markov models, support vector machines, optical flow i multimodal classification)
Offline learning metode

CNN (Convolutional Neural Network)

Ekstrahira prostorne znaÄajke iz slika/video koristeÄ‡i konvolucijske filtere.

Liquid State Machines (LSM)

Vrsta recurrent spiking neural network; pamti kratkoroÄne temporalne obrasce â†’ dobro za vremenske serije i neuromorfne spike podatke.

Restricted Boltzmann Machines (RBM)

ProbabilistiÄki model za uÄenje distribucija i reprezentacija podataka; Äesto se koristi za feature extraction i pretreniravanje dubljih mreÅ¾a.

Hidden Markov Models (HMM)

Model sekvencijalnih podataka gdje stanja nisu direktno vidljiva, ali generiraju opaÅ¾ene dogaÄ‘aje â†’ koristi se za govor, signale i vremenske serije.

Support Vector Machines (SVM)

Supervised learning metoda koja pronalazi optimalnu granicu izmeÄ‘u klasa u feature prostoru.

Optical Flow

Tehnika za praÄ‡enje kretanja objekata u video sekvencama prema promjeni intenziteta piksela kroz vrijeme.

Multimodal Classification

Kombinacija viÅ¡e tipova podataka (npr. video + audio + senzori) za preciznu klasifikaciju.
ÄŒip vrti te iste algoritme bez da ih treba modificirati

Za dodatno testiranje, razvijena je i dodatna multiobject detekcija i klasifikacija s fiksnom kamerom
Zadaci: detekcija ljudi, auti, i ostalih vozila na slikama gdje se pojavljuju rijetko s ciljem minimizacije false detekcija (sustav spikea samo kad se neÅ¡to stvarno pojavi; ne troÅ¡i eneriju bezveze)
Radi na 400x240 piksela. ÄŒip je troÅ¡io 63 mW na 30-fps videu u 3 boje koji je zatim bio skaliran na 1920x1080. Bilo je potrebno konvertirati piksele u spike evente jer je video bio snimljen standarndom kamerom
dobre performanse

MoguÄ‡e koristiti i spike-based retinal cameru
Implementirana je vizualna "mapa" orijentacijsko selektivnih filtera inspiriranih ranim procesiranjem u coretexu sisavaca koji se Äesto koristi u CV za feature extraction

Svih milijun neurona dobilo je feedforward inpute s orientation biasima iz vizualnog prostora skupa s rekurzivnim konekcijama meÄ‘u bliskim featuresima

Standardni benchmark u arhitekturi raÄunala je: energy per operation
U domeni konfigurabilnih neuralnih arhitektura, osnovna operacija je "sinaptiÄki event" koji predstavlja source neuron koji Å¡alje spike event target neuronu putem posebne (non-zero) sinapse
SinaptiÄki eventi su atomiÄne jedinice mjerenja jer se koliÄina raÄunanja, memorija, komunikacija, snaga, povrÅ¡ina i brzina proporcionalno poveÄ‡avaju s brojem sinapsi

KoristeÄ‡i rekurzivno poveane mreÅ¾e, izmjeren je total power i energy per synaptic event
Power consumption  je funkcija spike rate-a, prosjeÄne udaljenosti koju spikeovi prolaze i prosjeÄnog broja aktivnih sinapsi po neuronu (synaptic density)

Ako je "fire" u prosjeku 20 hz uz 128 aktivnih sinapsi, ukupna izmjerena snaga iznosila je 72mW (na naponu od 0.775 V), Å¡to odgovara 26pj po sinaptiÄkom eventu

U usporedbi s standarnim raÄunalom, troÅ¡i se 176000 x manje energije po eventu
OmoguÄ‡ava se individualno programiranje neurona i njihovih konekcijama
SPOS -> sinaptiÄke operacije po sekundi u ovom sustavu
Standardna superraÄunala -> FLOPS (floating point operations per second)

Slaganjem viÅ¡e TrueNorth brainchipova, mogu nastati superraÄunala sa stotinama tisuÄ‡a jezgri, stotinama milijuna neurona i stotinama milijardi sinapsi

Arhitektura -> slika -> svaki node je spojen sa svakim (node = core, edge = neural connection)

#####################################################################################################
NaÅ¡a san video SNN in 5 minutes...pa zaÅ¡ ne hitit oko
Mozak jako efikasno procesira informacije
SNNovi su novi tip AI inspiriran komunikacijom mozga
ObiÄne neuronske baziraju se na kontinuiranim signalima, SNNovi koriste spikeove kao mozak
Spikes => diskretni eventi koji se javljaju kroz vrijeme
RealitiÄniji i efikasniji za obradu temporalnih informacija (informacije koji nisu statiÄne, nego imaju vremensku "dimenziju"...videa, signali, Å¾ivÄani signali, eventi za event kamere i sl.)
Real time procesiranje, event based vision, robotika, autonomna vozila

Komponente SNN-ova:
- meÄ‘usobno povezani neuroni
- svaki neuron prima input od drugih
- inputi su elektriÄni impulsi koje zovemo "spikes"
Kad neuron primi dovoljno spikeova u kratkom periodu, "fire-a" svoj spike
Tako neuroni komuniciraju
Timing spikeova je kljuÄan => enkodira informacije o inputu
ÄŒa je jaÄi input, jaÄi je i spike
Sinapse - veze izmeÄ‘u neurona
Synaptic plasticity - sposobnost da sinapsa jaÄa ili slabi kroz vrijeme => tako se SNN-ovi adaptiraju i uÄe kroz vrijeme
Primjer: speÄemo se na peÄ‡, pain receptori poÅ¡alju spikeove mozgu, to pojaÄava vezu meÄ‘u neuronima zaduÅ¾enim za toÄno taj proces (osjet boli zbog vruÄ‡e peÄ‡i), iduÄ‡i put kad smo blizu vruÄ‡eg objekta, mozak Ä‡e brÅ¾e prepoznati opasnost i brÅ¾e Ä‡emo reagirati => synaptic plasticity IRL

Encoding information => kako pretvorit real-world data u spike patterne?
Pretvorba naÅ¡eg jezika u jezik neurona



Rate coding -> frequency spikeova predstavlja intenzitet inputa => Äa je input jaÄi, to ima viÅ¡e spikeova...tipa...video koji prikazuje high-speed let drona stvara viÅ¡e spikeova nego video bijelega zida na kojemu se niÅ¡ ne deÅ¡ava

NajklasiÄniji i najÄeÅ¡Ä‡e koriÅ¡ten model.

Frekvencija spikeova = intenzitet podraÅ¾aja.

Tvoj primjer s dronom je savrÅ¡en â€” ako kamera â€œvidiâ€ puno promjena â†’ neuroni Ä‡e ispucavati puno spikeova. Ako je scena statiÄna (bijeli zid), spikeova gotovo da nema.
ğŸ‘‰ OgraniÄenje: sporije reagira jer se Äeka da se skupi odreÄ‘eni broj spikeova da bi se dobila jasna slika intenziteta.

Temporal coding -> koristi se precizni timing spikeova za prijenos informacija. ToÄno vrijeme kad se spike dogodi moÅ¾e biti kljuÄno; posebno za taskove gdje je bitno vrijeme i praÄ‡enje sekvenci

Tu je vaÅ¾an toÄan trenutak kad spike doÄ‘e.

ÄŒak i ako je broj spikeova isti, razliÄit raspored u vremenu moÅ¾e nositi razliÄite informacije.

Super za event-based senzore i zadatke gdje redoslijed i precizno vrijeme imaju smisla (npr. prepoznavanje govora, ritma, ili toÄno kada je objekt uÅ¡ao u vidno polje).
ğŸ‘‰ Prednost: moÅ¾e biti ekstremno energetski efikasno, jer mali broj spikeova moÅ¾e nositi puno info.

Population coding -> koristi se grupa neurona da predstavlja 1 input. Svaki neuron u populaciji predstavlja specifiÄni feature inputa. Potpuni pattern aktivnosti kroz populaciju predstavlja potpunu informaciju; npr. razliÄiti neuroni mogu "fire-ati" za razliÄite boje unutar slike

Radi se o skupinama neurona â€” svaki neuron se â€œpaliâ€ za odreÄ‘eni feature (npr. specifiÄnu boju, kut linije, smjer kretanja).

Informacija se dobiva iz kolektivnog uzorka aktivnosti svih neurona.

Primjer: u vidnom korteksu mozga razliÄiti neuroni su specijalizirani za razliÄite orijentacije linija â†’ cijela populacija ti daje kompletnu reprezentaciju slike.
ğŸ‘‰ Ovo je â€œrobustnijeâ€ jer se informacija ne oslanja na jedan neuron nego na distribuciju aktivnosti kroz cijelu populaciju.

Neuron model
LIFT model => matematiÄki model toga kako neuroni procesiraju spikeove
Kanta s vodom => neuron
Kapljice vode => nadolazeÄ‡i spikeovi
Svaki spike dodaje malo vode u kantu/napona u neuron

Kanta ima malu Å¡kulju koja uzorkuje da sporo gubi vodu kroz vrijeme => to predstavlja slabljenje napona u neuronu s prolaskom vremena (ono kad na kameri event izblijedi)
Ako se kanta napuni i prelije, to je ekvivalentno temu kad neuron dobije dovoljno napona da postigne spike i resetira se  da bi mogao obraÄ‘ivati novu rundu inputa => neuron fireing

Moremo prilagoÄ‘avat koliko se voda gubi i potrebnu koliÄinu vode da se kanta napuni (dakle; tunable parametri su threshold za spike i vrijeme slabljenja eventa)

Za slaganje SNN-ova => neuroni + sinapse + definiranje kako sinapse uÄe + layeri neurona + broj neurona u layeru
